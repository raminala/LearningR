---
title: Cleaning Data in R
author: ''
date: '2021-09-18'
slug: cleaning-data-in-r
categories: []
tags:
  - data_visualization
  - data_wrangling
---

<script src="{{< blogdown/postref >}}index.en_files/header-attrs/header-attrs.js"></script>


<p>It’s commonly said that data scientists spend 80% of their time cleaning and manipulating data and only 20% of their time analyzing it. The time spent cleaning is vital since analyzing dirty data can lead you to draw inaccurate conclusions.</p>
<p>In this course, you’ll learn how to clean dirty data. Using R, you’ll learn how to identify values that don’t look right and fix dirty data by converting data types, filling in missing values, and using fuzzy string matching. As you learn, you’ll brush up on your skills by working with real-world datasets, including bike-share trips, customer asset portfolios, and restaurant reviews—developing the skills you need to go from raw data to awesome insights as quickly and accurately as possible!</p>
<div id="converting-data-types" class="section level2">
<h2>1-3 Converting data types</h2>
<p>Throughout this chapter, you’ll be working with San Francisco bike share ride data called bike_share_rides. It contains information on start and end stations of each trip, the trip duration, and some user information.</p>
<p>Before beginning to analyze any dataset, it’s important to take a look at the different types of columns you’ll be working with, which you can do using glimpse().</p>
<p>In this exercise, you’ll take a look at the data types contained in bike_share_rides and see how an incorrect data type can flaw your analysis.</p>
<p>dplyr and assertive are loaded and bike_share_rides is available.</p>
<pre class="r"><code># Glimpse at bike_share_rides
glimpse(bike_share_rides)</code></pre>
<pre><code>## Rows: 35,229
## Columns: 10
## $ ride_id         &lt;int&gt; 52797, 54540, 87695, 45619, 70832, 96135, 29928, 83331~
## $ date            &lt;chr&gt; &quot;2017-04-15&quot;, &quot;2017-04-19&quot;, &quot;2017-04-14&quot;, &quot;2017-04-03&quot;~
## $ duration        &lt;chr&gt; &quot;1316.15 minutes&quot;, &quot;8.13 minutes&quot;, &quot;24.85 minutes&quot;, &quot;6~
## $ station_A_id    &lt;dbl&gt; 67, 21, 16, 58, 16, 6, 5, 16, 5, 81, 30, 16, 16, 67, 2~
## $ station_A_name  &lt;chr&gt; &quot;San Francisco Caltrain Station 2  (Townsend St at 4th~
## $ station_B_id    &lt;dbl&gt; 89, 64, 355, 368, 81, 66, 350, 91, 62, 81, 109, 10, 80~
## $ station_B_name  &lt;chr&gt; &quot;Division St at Potrero Ave&quot;, &quot;5th St at Brannan St&quot;, ~
## $ bike_id         &lt;dbl&gt; 1974, 860, 2263, 1417, 507, 75, 388, 239, 1449, 3289, ~
## $ user_gender     &lt;chr&gt; &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;~
## $ user_birth_year &lt;dbl&gt; 1972, 1986, 1993, 1981, 1981, 1988, 1993, 1996, 1993, ~</code></pre>
<pre class="r"><code># Summary of user_birth_year
summary(bike_share_rides$user_birth_year)</code></pre>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##    1900    1979    1986    1984    1991    2001</code></pre>
<pre class="r"><code># Convert user_birth_year to factor: user_birth_year_fct
bike_share_rides &lt;- bike_share_rides %&gt;%
  mutate(user_birth_year_fct = factor(user_birth_year))

# Assert user_birth_year_fct is a factor
assert_is_factor(bike_share_rides$user_birth_year_fct)

# Summary of user_birth_year_fct
summary(bike_share_rides$user_birth_year_fct)</code></pre>
<pre><code>## 1900 1902 1923 1931 1938 1939 1941 1942 1943 1945 1946 1947 1948 1949 1950 1951 
##    1    7    2   23    2    1    3   10    4   16    5   24    9   30   37   25 
## 1952 1953 1954 1955 1956 1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 
##   70   49   65   66  112   62  156   99  196  161  256  237  245  349  225  363 
## 1968 1969 1970 1971 1972 1973 1974 1975 1976 1977 1978 1979 1980 1981 1982 1983 
##  365  331  370  548  529  527  563  601  481  541  775  876  825 1016 1056 1262 
## 1984 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 
## 1157 1318 1606 1672 2135 1872 2062 1582 1703 1498 1476 1185  813  358  365  348 
## 2000 2001 
##  473   30</code></pre>
</div>
<div id="trimming-strings" class="section level2">
<h2>1-4 Trimming strings</h2>
<p>In the previous exercise, you were able to identify the correct data type and convert user_birth_year to the correct type, allowing you to extract counts that gave you a bit more insight into the dataset.</p>
<p>Another common dirty data problem is having extra bits like percent signs or periods in numbers, causing them to be read in as characters. In order to be able to crunch these numbers, the extra bits need to be removed and the numbers need to be converted from character to numeric. In this exercise, you’ll need to convert the duration column from character to numeric, but before this can happen, the word “minutes” needs to be removed from each value.</p>
<p>dplyr, assertive, and stringr are loaded and bike_share_rides is available.</p>
<pre class="r"><code>bike_share_rides &lt;- bike_share_rides %&gt;%
  # Remove &#39;minutes&#39; from duration: duration_trimmed
  mutate(duration_trimmed = str_remove(duration, &quot;minutes&quot;),
         # Convert duration_trimmed to numeric: duration_mins
         duration_mins = as.numeric(duration_trimmed))

# Glimpse at bike_share_rides
glimpse(bike_share_rides)</code></pre>
<pre><code>## Rows: 35,229
## Columns: 13
## $ ride_id             &lt;int&gt; 52797, 54540, 87695, 45619, 70832, 96135, 29928, 8~
## $ date                &lt;chr&gt; &quot;2017-04-15&quot;, &quot;2017-04-19&quot;, &quot;2017-04-14&quot;, &quot;2017-04~
## $ duration            &lt;chr&gt; &quot;1316.15 minutes&quot;, &quot;8.13 minutes&quot;, &quot;24.85 minutes&quot;~
## $ station_A_id        &lt;dbl&gt; 67, 21, 16, 58, 16, 6, 5, 16, 5, 81, 30, 16, 16, 6~
## $ station_A_name      &lt;chr&gt; &quot;San Francisco Caltrain Station 2  (Townsend St at~
## $ station_B_id        &lt;dbl&gt; 89, 64, 355, 368, 81, 66, 350, 91, 62, 81, 109, 10~
## $ station_B_name      &lt;chr&gt; &quot;Division St at Potrero Ave&quot;, &quot;5th St at Brannan S~
## $ bike_id             &lt;dbl&gt; 1974, 860, 2263, 1417, 507, 75, 388, 239, 1449, 32~
## $ user_gender         &lt;chr&gt; &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;M~
## $ user_birth_year     &lt;dbl&gt; 1972, 1986, 1993, 1981, 1981, 1988, 1993, 1996, 19~
## $ user_birth_year_fct &lt;fct&gt; 1972, 1986, 1993, 1981, 1981, 1988, 1993, 1996, 19~
## $ duration_trimmed    &lt;chr&gt; &quot;1316.15 &quot;, &quot;8.13 &quot;, &quot;24.85 &quot;, &quot;6.35 &quot;, &quot;9.8 &quot;, &quot;1~
## $ duration_mins       &lt;dbl&gt; 1316.15, 8.13, 24.85, 6.35, 9.80, 17.47, 16.52, 14~</code></pre>
<pre class="r"><code># Assert duration_mins is numeric
assert_is_numeric(bike_share_rides$duration_mins)

# Calculate mean duration
mean(bike_share_rides$duration_mins)</code></pre>
<pre><code>## [1] 13.06214</code></pre>
</div>
<div id="ride-duration-constraints" class="section level2">
<h2>1-6 Ride duration constraints</h2>
<p>Values that are out of range can throw off an analysis, so it’s important to catch them early on. In this exercise, you’ll be examining the duration_min column more closely. Bikes are not allowed to be kept out for more than 24 hours, or 1440 minutes at a time, but issues with some of the bikes caused inaccurate recording of the time they were returned.</p>
<p>In this exercise, you’ll replace erroneous data with the range limit (1440 minutes), however, you could just as easily replace these values with NAs.</p>
<p>dplyr, assertive, and ggplot2 are loaded and bike_share_rides is available.</p>
<pre class="r"><code># Create breaks
breaks &lt;- c(min(bike_share_rides$duration_mins), 0, 1440, max(bike_share_rides$duration_mins))

# Create a histogram of duration_min
ggplot(bike_share_rides, aes(duration_mins)) +
  geom_histogram(breaks = breaks)</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<pre class="r"><code># duration_min_const: replace vals of duration_min &gt; 1440 with 1440
bike_share_rides &lt;- bike_share_rides %&gt;%
  mutate(duration_min_const = replace(duration_mins, duration_mins &gt; 1440, 1440))

# Make sure all values of duration_min_const are between 0 and 1440
assert_all_are_in_closed_range(bike_share_rides$duration_min_const, lower = 0, upper = 1440)</code></pre>
</div>
<div id="back-to-the-future" class="section level2">
<h2>1-7 Back to the future</h2>
<p>Something has gone wrong and it looks like you have data with dates from the future, which is way outside of the date range you expected to be working with. To fix this, you’ll need to remove any rides from the dataset that have a date in the future. Before you can do this, the date column needs to be converted from a character to a Date. Having these as Date objects will make it much easier to figure out which rides are from the future, since R makes it easy to check if one Date object is before (&lt;) or after (&gt;) another.</p>
<p>dplyr and assertive are loaded and bike_share_rides is available.</p>
<pre class="r"><code>library(lubridate)</code></pre>
<pre><code>## 
## Attaching package: &#39;lubridate&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:base&#39;:
## 
##     date, intersect, setdiff, union</code></pre>
<pre class="r"><code># Convert date to Date type
bike_share_rides &lt;- bike_share_rides %&gt;%
  mutate(date = as.Date(date))

# Make sure all dates are in the past
assert_all_are_in_past(bike_share_rides$date)

# Filter for rides that occurred before or on today&#39;s date
bike_share_rides_past &lt;- bike_share_rides %&gt;%
  filter(date &lt; today())

# Make sure all dates from bike_share_rides_past are in the past
assert_all_are_in_past(bike_share_rides_past$date)</code></pre>
</div>
<div id="full-duplicates" class="section level2">
<h2>1-9 Full duplicates</h2>
<p>You’ve been notified that an update has been made to the bike sharing data pipeline to make it more efficient, but that duplicates are more likely to be generated as a result. To make sure that you can continue using the same scripts to run your weekly analyses about ride statistics, you’ll need to ensure that any duplicates in the dataset are removed first.</p>
<p>When multiple rows of a data frame share the same values for all columns, they’re full duplicates of each other. Removing duplicates like this is important, since having the same value repeated multiple times can alter summary statistics like the mean and median. Each ride, including its ride_id should be unique.</p>
<p>dplyr is loaded and bike_share_rides is available.</p>
<pre class="r"><code># Count the number of full duplicates
sum(duplicated(bike_share_rides))</code></pre>
<pre><code>## [1] 0</code></pre>
<pre class="r"><code># Remove duplicates
bike_share_rides_unique &lt;- filter(distinct(bike_share_rides))

# Count the full duplicates in bike_share_rides_unique
sum(duplicated(bike_share_rides_unique))</code></pre>
<pre><code>## [1] 0</code></pre>
</div>
<div id="removing-partial-duplicates" class="section level2">
<h2>1-10 Removing partial duplicates</h2>
<p>Now that you’ve identified and removed the full duplicates, it’s time to check for partial duplicates. Partial duplicates are a bit tricker to deal with than full duplicates. In this exercise, you’ll first identify any partial duplicates and then practice the most common technique to deal with them, which involves dropping all partial duplicates, keeping only the first.</p>
<p>dplyr is loaded and bike_share_rides is available.</p>
<pre class="r"><code># Find duplicated ride_ids
bike_share_rides %&gt;% 
  # Count the number of occurrences of each ride_id
  count(ride_id) %&gt;% 
  # Filter for rows with a count &gt; 1
  filter(n &gt; 1)</code></pre>
<pre><code>## # A tibble: 0 x 2
## # ... with 2 variables: ride_id &lt;int&gt;, n &lt;int&gt;</code></pre>
<pre class="r"><code># Remove full and partial duplicates
bike_share_rides_unique &lt;- bike_share_rides %&gt;%
  # Only based on ride_id instead of all cols
  distinct(ride_id, .keep_all = TRUE)

# Find duplicated ride_ids in bike_share_rides_unique
bike_share_rides_unique %&gt;%
  # Count the number of occurrences of each ride_id
  count(ride_id) %&gt;%
  # Filter for rows with a count &gt; 1
  filter(n &gt; 1)</code></pre>
<pre><code>## # A tibble: 0 x 2
## # ... with 2 variables: ride_id &lt;int&gt;, n &lt;int&gt;</code></pre>
</div>
<div id="aggregating-partial-duplicates" class="section level2">
<h2>1-11 Aggregating partial duplicates</h2>
<p>Another way of handling partial duplicates is to compute a summary statistic of the values that differ between partial duplicates, such as mean, median, maximum, or minimum. This can come in handy when you’re not sure how your data was collected and want an average, or if based on domain knowledge, you’d rather have too high of an estimate than too low of an estimate (or vice versa).</p>
<pre class="r"><code>bike_share_rides %&gt;%
  # Group by ride_id and date
  group_by(ride_id, date) %&gt;%
  # Add duration_min_avg column
  mutate(duration_min_avg = mean(duration_mins) ) %&gt;%
  # Remove duplicates based on ride_id and date, keep all cols
  distinct(ride_id, date, .keep_all = TRUE) %&gt;%
  # Remove duration_min column
  select(-duration_mins)</code></pre>
<pre><code>## # A tibble: 35,229 x 14
## # Groups:   ride_id, date [35,229]
##    ride_id date       duration   station_A_id station_A_name        station_B_id
##      &lt;int&gt; &lt;date&gt;     &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;                        &lt;dbl&gt;
##  1   52797 2017-04-15 1316.15 m~           67 San Francisco Caltra~           89
##  2   54540 2017-04-19 8.13 minu~           21 Montgomery St BART S~           64
##  3   87695 2017-04-14 24.85 min~           16 Steuart St at Market~          355
##  4   45619 2017-04-03 6.35 minu~           58 Market St at 10th St           368
##  5   70832 2017-04-10 9.8 minut~           16 Steuart St at Market~           81
##  6   96135 2017-04-18 17.47 min~            6 The Embarcadero at S~           66
##  7   29928 2017-04-22 16.52 min~            5 Powell St BART Stati~          350
##  8   83331 2017-04-11 14.72 min~           16 Steuart St at Market~           91
##  9   72424 2017-04-05 4.12 minu~            5 Powell St BART Stati~           62
## 10   25910 2017-04-20 25.77 min~           81 Berry St at 4th St              81
## # ... with 35,219 more rows, and 8 more variables: station_B_name &lt;chr&gt;,
## #   bike_id &lt;dbl&gt;, user_gender &lt;chr&gt;, user_birth_year &lt;dbl&gt;,
## #   user_birth_year_fct &lt;fct&gt;, duration_trimmed &lt;chr&gt;,
## #   duration_min_const &lt;dbl&gt;, duration_min_avg &lt;dbl&gt;</code></pre>
</div>
<div id="not-a-member" class="section level2">
<h2>2-3 Not a member</h2>
<p>Now that you’ve practiced identifying membership constraint problems, it’s time to fix these problems in a new dataset. Throughout this chapter, you’ll be working with a dataset called sfo_survey, containing survey responses from passengers taking flights from San Francisco International Airport (SFO). Participants were asked questions about the airport’s cleanliness, wait times, safety, and their overall satisfaction.</p>
<p>There were a few issues during data collection that resulted in some inconsistencies in the dataset. In this exercise, you’ll be working with the dest_size column, which categorizes the size of the destination airport that the passengers were flying to. A data frame called dest_sizes is available that contains all the possible destination sizes. Your mission is to find rows with invalid dest_sizes and remove them from the data frame.</p>
<pre class="r"><code># Count the number of occurrences of dest_size
sfo_survey %&gt;%
  count(dest_size)</code></pre>
<pre><code>##   dest_size    n
## 1   Small      1
## 2       Hub    1
## 3       Hub 1756
## 4     Large  143
## 5   Large      1
## 6    Medium  682
## 7     Small  225</code></pre>
<pre class="r"><code># Find bad dest_size rows
sfo_survey %&gt;% 
  # Join with dest_sizes data frame to get bad dest_size rows
   anti_join(dest_sizes, by = &quot;dest_size&quot;) %&gt;%
  # Select id, airline, destination, and dest_size cols
  select(id, airline, destination, dest_size)</code></pre>
<pre><code>##     id     airline       destination dest_size
## 1  982   LUFTHANSA            MUNICH       Hub
## 2 2063    AMERICAN      PHILADELPHIA   Large  
## 3  777 UNITED INTL SAN JOSE DEL CABO   Small</code></pre>
<pre class="r"><code># Remove bad dest_size rows
sfo_survey %&gt;% 
  # Join with dest_sizes
  semi_join(dest_sizes, by = &quot;dest_size&quot;) %&gt;%
  # Count the number of each dest_size
  count(dest_size)</code></pre>
<pre><code>##   dest_size    n
## 1       Hub 1756
## 2     Large  143
## 3    Medium  682
## 4     Small  225</code></pre>
</div>
<div id="section" class="section level2">
<h2>2-5</h2>
</div>
<div id="section-1" class="section level2">
<h2>2-6</h2>
</div>
<div id="section-2" class="section level2">
<h2>2-7</h2>
</div>
<div id="section-3" class="section level2">
<h2>2-9</h2>
</div>
<div id="section-4" class="section level2">
<h2>2-10</h2>
</div>
<div id="section-5" class="section level2">
<h2>2-11</h2>
</div>
