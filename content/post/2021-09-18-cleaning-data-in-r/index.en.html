---
title: Cleaning Data in R
author: ''
date: '2021-09-18'
slug: cleaning-data-in-r
categories: []
tags:
  - data_visualization
  - data_wrangling
---

<script src="{{< blogdown/postref >}}index.en_files/header-attrs/header-attrs.js"></script>


<p>It’s commonly said that data scientists spend 80% of their time cleaning and manipulating data and only 20% of their time analyzing it. The time spent cleaning is vital since analyzing dirty data can lead you to draw inaccurate conclusions.</p>
<p>In this course, you’ll learn how to clean dirty data. Using R, you’ll learn how to identify values that don’t look right and fix dirty data by converting data types, filling in missing values, and using fuzzy string matching. As you learn, you’ll brush up on your skills by working with real-world datasets, including bike-share trips, customer asset portfolios, and restaurant reviews—developing the skills you need to go from raw data to awesome insights as quickly and accurately as possible!</p>
<div id="converting-data-types" class="section level2">
<h2>1-3 Converting data types</h2>
<p>Throughout this chapter, you’ll be working with San Francisco bike share ride data called bike_share_rides. It contains information on start and end stations of each trip, the trip duration, and some user information.</p>
<p>Before beginning to analyze any dataset, it’s important to take a look at the different types of columns you’ll be working with, which you can do using glimpse().</p>
<p>In this exercise, you’ll take a look at the data types contained in bike_share_rides and see how an incorrect data type can flaw your analysis.</p>
<p>dplyr and assertive are loaded and bike_share_rides is available.</p>
<pre class="r"><code># Glimpse at bike_share_rides
glimpse(bike_share_rides)</code></pre>
<pre><code>## Rows: 35,229
## Columns: 10
## $ ride_id         &lt;int&gt; 52797, 54540, 87695, 45619, 70832, 96135, 29928, 83331~
## $ date            &lt;chr&gt; &quot;2017-04-15&quot;, &quot;2017-04-19&quot;, &quot;2017-04-14&quot;, &quot;2017-04-03&quot;~
## $ duration        &lt;chr&gt; &quot;1316.15 minutes&quot;, &quot;8.13 minutes&quot;, &quot;24.85 minutes&quot;, &quot;6~
## $ station_A_id    &lt;dbl&gt; 67, 21, 16, 58, 16, 6, 5, 16, 5, 81, 30, 16, 16, 67, 2~
## $ station_A_name  &lt;chr&gt; &quot;San Francisco Caltrain Station 2  (Townsend St at 4th~
## $ station_B_id    &lt;dbl&gt; 89, 64, 355, 368, 81, 66, 350, 91, 62, 81, 109, 10, 80~
## $ station_B_name  &lt;chr&gt; &quot;Division St at Potrero Ave&quot;, &quot;5th St at Brannan St&quot;, ~
## $ bike_id         &lt;dbl&gt; 1974, 860, 2263, 1417, 507, 75, 388, 239, 1449, 3289, ~
## $ user_gender     &lt;chr&gt; &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;~
## $ user_birth_year &lt;dbl&gt; 1972, 1986, 1993, 1981, 1981, 1988, 1993, 1996, 1993, ~</code></pre>
<pre class="r"><code># Summary of user_birth_year
summary(bike_share_rides$user_birth_year)</code></pre>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##    1900    1979    1986    1984    1991    2001</code></pre>
<pre class="r"><code># Convert user_birth_year to factor: user_birth_year_fct
bike_share_rides &lt;- bike_share_rides %&gt;%
  mutate(user_birth_year_fct = factor(user_birth_year))

# Assert user_birth_year_fct is a factor
assert_is_factor(bike_share_rides$user_birth_year_fct)

# Summary of user_birth_year_fct
summary(bike_share_rides$user_birth_year_fct)</code></pre>
<pre><code>## 1900 1902 1923 1931 1938 1939 1941 1942 1943 1945 1946 1947 1948 1949 1950 1951 
##    1    7    2   23    2    1    3   10    4   16    5   24    9   30   37   25 
## 1952 1953 1954 1955 1956 1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 
##   70   49   65   66  112   62  156   99  196  161  256  237  245  349  225  363 
## 1968 1969 1970 1971 1972 1973 1974 1975 1976 1977 1978 1979 1980 1981 1982 1983 
##  365  331  370  548  529  527  563  601  481  541  775  876  825 1016 1056 1262 
## 1984 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 
## 1157 1318 1606 1672 2135 1872 2062 1582 1703 1498 1476 1185  813  358  365  348 
## 2000 2001 
##  473   30</code></pre>
</div>
<div id="trimming-strings" class="section level2">
<h2>1-4 Trimming strings</h2>
<p>In the previous exercise, you were able to identify the correct data type and convert user_birth_year to the correct type, allowing you to extract counts that gave you a bit more insight into the dataset.</p>
<p>Another common dirty data problem is having extra bits like percent signs or periods in numbers, causing them to be read in as characters. In order to be able to crunch these numbers, the extra bits need to be removed and the numbers need to be converted from character to numeric. In this exercise, you’ll need to convert the duration column from character to numeric, but before this can happen, the word “minutes” needs to be removed from each value.</p>
<p>dplyr, assertive, and stringr are loaded and bike_share_rides is available.</p>
<pre class="r"><code>bike_share_rides &lt;- bike_share_rides %&gt;%
  # Remove &#39;minutes&#39; from duration: duration_trimmed
  mutate(duration_trimmed = str_remove(duration, &quot;minutes&quot;),
         # Convert duration_trimmed to numeric: duration_mins
         duration_mins = as.numeric(duration_trimmed))

# Glimpse at bike_share_rides
glimpse(bike_share_rides)</code></pre>
<pre><code>## Rows: 35,229
## Columns: 13
## $ ride_id             &lt;int&gt; 52797, 54540, 87695, 45619, 70832, 96135, 29928, 8~
## $ date                &lt;chr&gt; &quot;2017-04-15&quot;, &quot;2017-04-19&quot;, &quot;2017-04-14&quot;, &quot;2017-04~
## $ duration            &lt;chr&gt; &quot;1316.15 minutes&quot;, &quot;8.13 minutes&quot;, &quot;24.85 minutes&quot;~
## $ station_A_id        &lt;dbl&gt; 67, 21, 16, 58, 16, 6, 5, 16, 5, 81, 30, 16, 16, 6~
## $ station_A_name      &lt;chr&gt; &quot;San Francisco Caltrain Station 2  (Townsend St at~
## $ station_B_id        &lt;dbl&gt; 89, 64, 355, 368, 81, 66, 350, 91, 62, 81, 109, 10~
## $ station_B_name      &lt;chr&gt; &quot;Division St at Potrero Ave&quot;, &quot;5th St at Brannan S~
## $ bike_id             &lt;dbl&gt; 1974, 860, 2263, 1417, 507, 75, 388, 239, 1449, 32~
## $ user_gender         &lt;chr&gt; &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;M~
## $ user_birth_year     &lt;dbl&gt; 1972, 1986, 1993, 1981, 1981, 1988, 1993, 1996, 19~
## $ user_birth_year_fct &lt;fct&gt; 1972, 1986, 1993, 1981, 1981, 1988, 1993, 1996, 19~
## $ duration_trimmed    &lt;chr&gt; &quot;1316.15 &quot;, &quot;8.13 &quot;, &quot;24.85 &quot;, &quot;6.35 &quot;, &quot;9.8 &quot;, &quot;1~
## $ duration_mins       &lt;dbl&gt; 1316.15, 8.13, 24.85, 6.35, 9.80, 17.47, 16.52, 14~</code></pre>
<pre class="r"><code># Assert duration_mins is numeric
assert_is_numeric(bike_share_rides$duration_mins)

# Calculate mean duration
mean(bike_share_rides$duration_mins)</code></pre>
<pre><code>## [1] 13.06214</code></pre>
</div>
<div id="ride-duration-constraints" class="section level2">
<h2>1-6 Ride duration constraints</h2>
<p>Values that are out of range can throw off an analysis, so it’s important to catch them early on. In this exercise, you’ll be examining the duration_min column more closely. Bikes are not allowed to be kept out for more than 24 hours, or 1440 minutes at a time, but issues with some of the bikes caused inaccurate recording of the time they were returned.</p>
<p>In this exercise, you’ll replace erroneous data with the range limit (1440 minutes), however, you could just as easily replace these values with NAs.</p>
<p>dplyr, assertive, and ggplot2 are loaded and bike_share_rides is available.</p>
<pre class="r"><code># Create breaks
breaks &lt;- c(min(bike_share_rides$duration_mins), 0, 1440, max(bike_share_rides$duration_mins))

# Create a histogram of duration_min
ggplot(bike_share_rides, aes(duration_mins)) +
  geom_histogram(breaks = breaks)</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<pre class="r"><code># duration_min_const: replace vals of duration_min &gt; 1440 with 1440
bike_share_rides &lt;- bike_share_rides %&gt;%
  mutate(duration_min_const = replace(duration_mins, duration_mins &gt; 1440, 1440))

# Make sure all values of duration_min_const are between 0 and 1440
assert_all_are_in_closed_range(bike_share_rides$duration_min_const, lower = 0, upper = 1440)</code></pre>
</div>
<div id="back-to-the-future" class="section level2">
<h2>1-7 Back to the future</h2>
<p>Something has gone wrong and it looks like you have data with dates from the future, which is way outside of the date range you expected to be working with. To fix this, you’ll need to remove any rides from the dataset that have a date in the future. Before you can do this, the date column needs to be converted from a character to a Date. Having these as Date objects will make it much easier to figure out which rides are from the future, since R makes it easy to check if one Date object is before (&lt;) or after (&gt;) another.</p>
<p>dplyr and assertive are loaded and bike_share_rides is available.</p>
<pre class="r"><code>library(lubridate)
# Convert date to Date type
bike_share_rides &lt;- bike_share_rides %&gt;%
  mutate(date = as.Date(date))

# Make sure all dates are in the past
assert_all_are_in_past(bike_share_rides$date)

# Filter for rides that occurred before or on today&#39;s date
bike_share_rides_past &lt;- bike_share_rides %&gt;%
  filter(date &lt; today())

# Make sure all dates from bike_share_rides_past are in the past
assert_all_are_in_past(bike_share_rides_past$date)</code></pre>
</div>
<div id="full-duplicates" class="section level2">
<h2>1-9 Full duplicates</h2>
<p>You’ve been notified that an update has been made to the bike sharing data pipeline to make it more efficient, but that duplicates are more likely to be generated as a result. To make sure that you can continue using the same scripts to run your weekly analyses about ride statistics, you’ll need to ensure that any duplicates in the dataset are removed first.</p>
<p>When multiple rows of a data frame share the same values for all columns, they’re full duplicates of each other. Removing duplicates like this is important, since having the same value repeated multiple times can alter summary statistics like the mean and median. Each ride, including its ride_id should be unique.</p>
<p>dplyr is loaded and bike_share_rides is available.</p>
<pre class="r"><code># Count the number of full duplicates
sum(duplicated(bike_share_rides))</code></pre>
<pre><code>## [1] 0</code></pre>
<pre class="r"><code># Remove duplicates
bike_share_rides_unique &lt;- filter(distinct(bike_share_rides))

# Count the full duplicates in bike_share_rides_unique
sum(duplicated(bike_share_rides_unique))</code></pre>
<pre><code>## [1] 0</code></pre>
</div>
<div id="removing-partial-duplicates" class="section level2">
<h2>1-10 Removing partial duplicates</h2>
<p>Now that you’ve identified and removed the full duplicates, it’s time to check for partial duplicates. Partial duplicates are a bit tricker to deal with than full duplicates. In this exercise, you’ll first identify any partial duplicates and then practice the most common technique to deal with them, which involves dropping all partial duplicates, keeping only the first.</p>
<p>dplyr is loaded and bike_share_rides is available.</p>
<pre class="r"><code># Find duplicated ride_ids
bike_share_rides %&gt;% 
  # Count the number of occurrences of each ride_id
  count(ride_id) %&gt;% 
  # Filter for rows with a count &gt; 1
  filter(n &gt; 1)</code></pre>
<pre><code>## # A tibble: 0 x 2
## # ... with 2 variables: ride_id &lt;int&gt;, n &lt;int&gt;</code></pre>
<pre class="r"><code># Remove full and partial duplicates
bike_share_rides_unique &lt;- bike_share_rides %&gt;%
  # Only based on ride_id instead of all cols
  distinct(ride_id, .keep_all = TRUE)

# Find duplicated ride_ids in bike_share_rides_unique
bike_share_rides_unique %&gt;%
  # Count the number of occurrences of each ride_id
  count(ride_id) %&gt;%
  # Filter for rows with a count &gt; 1
  filter(n &gt; 1)</code></pre>
<pre><code>## # A tibble: 0 x 2
## # ... with 2 variables: ride_id &lt;int&gt;, n &lt;int&gt;</code></pre>
</div>
<div id="aggregating-partial-duplicates" class="section level2">
<h2>1-11 Aggregating partial duplicates</h2>
<p>Another way of handling partial duplicates is to compute a summary statistic of the values that differ between partial duplicates, such as mean, median, maximum, or minimum. This can come in handy when you’re not sure how your data was collected and want an average, or if based on domain knowledge, you’d rather have too high of an estimate than too low of an estimate (or vice versa).</p>
<pre class="r"><code>bike_share_rides %&gt;%
  # Group by ride_id and date
  group_by(ride_id, date) %&gt;%
  # Add duration_min_avg column
  mutate(duration_min_avg = mean(duration_mins) ) %&gt;%
  # Remove duplicates based on ride_id and date, keep all cols
  distinct(ride_id, date, .keep_all = TRUE) %&gt;%
  # Remove duration_min column
  select(-duration_mins)</code></pre>
<pre><code>## # A tibble: 35,229 x 14
## # Groups:   ride_id, date [35,229]
##    ride_id date       duration   station_A_id station_A_name        station_B_id
##      &lt;int&gt; &lt;date&gt;     &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;                        &lt;dbl&gt;
##  1   52797 2017-04-15 1316.15 m~           67 San Francisco Caltra~           89
##  2   54540 2017-04-19 8.13 minu~           21 Montgomery St BART S~           64
##  3   87695 2017-04-14 24.85 min~           16 Steuart St at Market~          355
##  4   45619 2017-04-03 6.35 minu~           58 Market St at 10th St           368
##  5   70832 2017-04-10 9.8 minut~           16 Steuart St at Market~           81
##  6   96135 2017-04-18 17.47 min~            6 The Embarcadero at S~           66
##  7   29928 2017-04-22 16.52 min~            5 Powell St BART Stati~          350
##  8   83331 2017-04-11 14.72 min~           16 Steuart St at Market~           91
##  9   72424 2017-04-05 4.12 minu~            5 Powell St BART Stati~           62
## 10   25910 2017-04-20 25.77 min~           81 Berry St at 4th St              81
## # ... with 35,219 more rows, and 8 more variables: station_B_name &lt;chr&gt;,
## #   bike_id &lt;dbl&gt;, user_gender &lt;chr&gt;, user_birth_year &lt;dbl&gt;,
## #   user_birth_year_fct &lt;fct&gt;, duration_trimmed &lt;chr&gt;,
## #   duration_min_const &lt;dbl&gt;, duration_min_avg &lt;dbl&gt;</code></pre>
</div>
<div id="not-a-member" class="section level2">
<h2>2-3 Not a member</h2>
<p>Now that you’ve practiced identifying membership constraint problems, it’s time to fix these problems in a new dataset. Throughout this chapter, you’ll be working with a dataset called sfo_survey, containing survey responses from passengers taking flights from San Francisco International Airport (SFO). Participants were asked questions about the airport’s cleanliness, wait times, safety, and their overall satisfaction.</p>
<p>There were a few issues during data collection that resulted in some inconsistencies in the dataset. In this exercise, you’ll be working with the dest_size column, which categorizes the size of the destination airport that the passengers were flying to. A data frame called dest_sizes is available that contains all the possible destination sizes. Your mission is to find rows with invalid dest_sizes and remove them from the data frame.</p>
<pre class="r"><code># Count the number of occurrences of dest_size
sfo_survey %&gt;%
  count(dest_size)</code></pre>
<pre><code>##   dest_size    n
## 1   Small      1
## 2       Hub    1
## 3       Hub 1756
## 4     Large  143
## 5   Large      1
## 6    Medium  682
## 7     Small  225</code></pre>
<pre class="r"><code># Find bad dest_size rows
sfo_survey %&gt;% 
  # Join with dest_sizes data frame to get bad dest_size rows
   anti_join(dest_sizes, by = &quot;dest_size&quot;) %&gt;%
  # Select id, airline, destination, and dest_size cols
  select(id, airline, destination, dest_size)</code></pre>
<pre><code>##     id     airline       destination dest_size
## 1  982   LUFTHANSA            MUNICH       Hub
## 2 2063    AMERICAN      PHILADELPHIA   Large  
## 3  777 UNITED INTL SAN JOSE DEL CABO   Small</code></pre>
<pre class="r"><code># Remove bad dest_size rows
sfo_survey %&gt;% 
  # Join with dest_sizes
  semi_join(dest_sizes, by = &quot;dest_size&quot;) %&gt;%
  # Count the number of each dest_size
  count(dest_size)</code></pre>
<pre><code>##   dest_size    n
## 1       Hub 1756
## 2     Large  143
## 3    Medium  682
## 4     Small  225</code></pre>
</div>
<div id="identifying-inconsistency" class="section level2">
<h2>2-5 Identifying inconsistency</h2>
<p>In the video exercise, you learned about different kinds of inconsistencies that can occur within categories, making it look like a variable has more categories than it should.</p>
<p>In this exercise, you’ll continue working with the sfo_survey dataset. You’ll examine the dest_size column again as well as the cleanliness column and determine what kind of issues, if any, these two categorical variables face.</p>
<pre class="r"><code># Count dest_size
sfo_survey %&gt;%
  count(dest_size)</code></pre>
<pre><code>##   dest_size    n
## 1   Small      1
## 2       Hub    1
## 3       Hub 1756
## 4     Large  143
## 5   Large      1
## 6    Medium  682
## 7     Small  225</code></pre>
<pre class="r"><code># Count cleanliness
sfo_survey %&gt;%
  count(cleanliness)</code></pre>
<pre><code>##      cleanliness    n
## 1        Average  433
## 2          Clean  970
## 3          Dirty    2
## 4 Somewhat clean 1254
## 5 Somewhat dirty   30
## 6           &lt;NA&gt;  120</code></pre>
</div>
<div id="correcting-inconsistency" class="section level2">
<h2>2-6 Correcting inconsistency</h2>
<p>Now that you’ve identified that dest_size has whitespace inconsistencies and cleanliness has capitalization inconsistencies, you’ll use the new tools at your disposal to fix the inconsistent values in sfo_survey instead of removing the data points entirely, which could add bias to your dataset if more than 5% of the data points need to be dropped.</p>
<pre class="r"><code># Add new columns to sfo_survey
sfo_survey &lt;- sfo_survey %&gt;%
  # dest_size_trimmed: dest_size without whitespace
  mutate(dest_size_trimmed = str_trim(dest_size),
         # cleanliness_lower: cleanliness converted to lowercase
         cleanliness_lower = str_to_lower(cleanliness))

# Count values of dest_size_trimmed
sfo_survey %&gt;%
  count(dest_size_trimmed)</code></pre>
<pre><code>##   dest_size_trimmed    n
## 1               Hub 1757
## 2             Large  144
## 3            Medium  682
## 4             Small  226</code></pre>
<pre class="r"><code># Count values of cleanliness_lower
sfo_survey %&gt;%
  count(cleanliness_lower)</code></pre>
<pre><code>##   cleanliness_lower    n
## 1           average  433
## 2             clean  970
## 3             dirty    2
## 4    somewhat clean 1254
## 5    somewhat dirty   30
## 6              &lt;NA&gt;  120</code></pre>
</div>
<div id="collapsing-categories" class="section level2">
<h2>2-7 Collapsing categories</h2>
<p>One of the tablets that participants filled out the sfo_survey on was not properly configured, allowing the response for dest_region to be free text instead of a dropdown menu. This resulted in some inconsistencies in the dest_region variable that you’ll need to correct in this exercise to ensure that the numbers you report to your boss are as accurate as possible.</p>
<p>dplyr and forcats are loaded and sfo_survey is available.</p>
<pre class="r"><code># Count categories of dest_region
sfo_survey %&gt;%
  count(dest_region)</code></pre>
<pre><code>##             dest_region   n
## 1                  Asia 260
## 2 Australia/New Zealand  66
## 3         Canada/Mexico 220
## 4 Central/South America  29
## 5               East US 498
## 6                Europe 401
## 7           Middle East  79
## 8            Midwest US 281
## 9               West US 975</code></pre>
<pre class="r"><code># Categories to map to Europe
europe_categories &lt;- c(&quot;EU&quot;, &quot;Europ&quot;, &quot;eur&quot;)

# Add a new col dest_region_collapsed
sfo_survey %&gt;%
  # Map all categories in europe_categories to Europe
  mutate(dest_region_collapsed = fct_collapse(dest_region, 
                                     Europe = europe_categories)) %&gt;%
  # Count categories of dest_region_collapsed
  count(dest_region_collapsed)</code></pre>
<pre><code>## Warning: Unknown levels in `f`: EU, Europ, eur</code></pre>
<pre><code>##   dest_region_collapsed   n
## 1                  Asia 260
## 2 Australia/New Zealand  66
## 3         Canada/Mexico 220
## 4 Central/South America  29
## 5               East US 498
## 6                Europe 401
## 7           Middle East  79
## 8            Midwest US 281
## 9               West US 975</code></pre>
</div>
<div id="detecting-inconsistent-text-data" class="section level2">
<h2>2-9 Detecting inconsistent text data</h2>
<p>You’ve recently received some news that the customer support team wants to ask the SFO survey participants some follow-up questions. However, the auto-dialer that the call center uses isn’t able to parse all of the phone numbers since they’re all in different formats. After some investigation, you found that some phone numbers are written with hyphens (-) and some are written with parentheses ((,)). In this exercise, you’ll figure out which phone numbers have these issues so that you know which ones need fixing.</p>
<p>dplyr and stringr are loaded, and phone_correction is available.</p>
<pre class="r"><code># Filter for rows with &quot;-&quot; in the phone column
phone_correction %&gt;%
  filter(str_detect(phone, &quot;-&quot;))</code></pre>
<pre><code>## # A tibble: 1,421 x 4
##       id airline          destination          phone         
##    &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;                &lt;chr&gt;         
##  1  1844 TURKISH AIRLINES ISTANBUL             731-813-2043  
##  2  1840 TURKISH AIRLINES ISTANBUL             563-732-6802  
##  3  3010 AMERICAN         MIAMI                (637) 782-6989
##  4  2097 UNITED INTL      MEXICO CITY          (359) 803-9809
##  5  1835 TURKISH AIRLINES ISTANBUL             (416) 788-2844
##  6  1849 TURKISH AIRLINES ISTANBUL             311-305-4367  
##  7  2289 QANTAS           SYDNEY               817-400-0481  
##  8   105 UNITED           WASHINGTON DC-DULLES (729) 609-4819
##  9  1973 CATHAY PACIFIC   HONG KONG            (201) 737-4409
## 10  2385 UNITED INTL      SYDNEY               (137) 611-3694
## # ... with 1,411 more rows</code></pre>
<pre class="r"><code># Filter for rows with &quot;(&quot; or &quot;)&quot; in the phone column
phone_correction %&gt;%
  filter(str_detect(phone, fixed(&quot;(&quot;)) | str_detect(phone, fixed(&quot;)&quot;)))</code></pre>
<pre><code>## # A tibble: 739 x 4
##       id airline          destination          phone         
##    &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;                &lt;chr&gt;         
##  1  3010 AMERICAN         MIAMI                (637) 782-6989
##  2  2097 UNITED INTL      MEXICO CITY          (359) 803-9809
##  3  1835 TURKISH AIRLINES ISTANBUL             (416) 788-2844
##  4   105 UNITED           WASHINGTON DC-DULLES (729) 609-4819
##  5  1973 CATHAY PACIFIC   HONG KONG            (201) 737-4409
##  6  2385 UNITED INTL      SYDNEY               (137) 611-3694
##  7   517 UNITED           FT. LAUDERDALE       (812) 869-6263
##  8  2885 EVA AIR          TAIPEI               (194) 198-0504
##  9  2128 FRONTIER         DENVER               (299) 137-6993
## 10  2132 FRONTIER         DENVER               (739) 710-2966
## # ... with 729 more rows</code></pre>
</div>
<div id="replacing-and-removing" class="section level2">
<h2>2-10 Replacing and removing</h2>
<p>In the last exercise, you saw that the phone column of sfo_data is plagued with unnecessary parentheses and hyphens. The customer support team has requested that all phone numbers be in the format “123 456 7890”. In this exercise, you’ll use your new stringr skills to fulfill this request.</p>
<pre class="r"><code># Remove parentheses from phone column
phone_no_parens &lt;- phone_correction$phone %&gt;%
  # Remove &quot;(&quot;s
  str_remove_all(fixed(&quot;(&quot;)) %&gt;%
  # Remove &quot;)&quot;s
  str_remove_all(fixed(&quot;)&quot;))

# Add phone_no_parens as column
phone_correction %&gt;%
  mutate(phone_no_parens = phone_no_parens,
  # Replace all hyphens in phone_no_parens with spaces
         phone_clean = str_replace_all(phone_no_parens, &quot;-&quot;, &quot; &quot;))</code></pre>
<pre><code>## # A tibble: 2,809 x 6
##       id airline          destination phone          phone_no_parens phone_clean
##    &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;       &lt;chr&gt;          &lt;chr&gt;           &lt;chr&gt;      
##  1  1842 TURKISH AIRLINES ISTANBUL    858 990 5153   858 990 5153    858 990 51~
##  2  1844 TURKISH AIRLINES ISTANBUL    731-813-2043   731-813-2043    731 813 20~
##  3  1840 TURKISH AIRLINES ISTANBUL    563-732-6802   563-732-6802    563 732 68~
##  4  1837 TURKISH AIRLINES ISTANBUL    145 725 4021   145 725 4021    145 725 40~
##  5  1833 TURKISH AIRLINES ISTANBUL    931 311 5801   931 311 5801    931 311 58~
##  6  3010 AMERICAN         MIAMI       (637) 782-6989 637 782-6989    637 782 69~
##  7  1838 TURKISH AIRLINES ISTANBUL    172 990 3485   172 990 3485    172 990 34~
##  8  1845 TURKISH AIRLINES ISTANBUL    872 325 4341   872 325 4341    872 325 43~
##  9  2097 UNITED INTL      MEXICO CITY (359) 803-9809 359 803-9809    359 803 98~
## 10  1846 TURKISH AIRLINES ISTANBUL    152 790 8238   152 790 8238    152 790 82~
## # ... with 2,799 more rows</code></pre>
</div>
<div id="invalid-phone-numbers" class="section level2">
<h2>2-11 Invalid phone numbers</h2>
<p>The customer support team is grateful for your work so far, but during their first day of calling participants, they ran into some phone numbers that were invalid. In this exercise, you’ll remove any rows with invalid phone numbers so that these faulty numbers don’t keep slowing the team down.</p>
<p>Examine the invalid phone numbers by filtering for numbers whose length is not equal to 12.
Remove the rows with invalid numbers by filtering for numbers with a length of exactly 12.</p>
<pre class="r"><code># Check out the invalid numbers
phone_correction %&gt;%
  filter(str_length(phone) != 12)</code></pre>
<pre><code>## # A tibble: 744 x 4
##       id airline          destination          phone         
##    &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;                &lt;chr&gt;         
##  1  3010 AMERICAN         MIAMI                (637) 782-6989
##  2  2097 UNITED INTL      MEXICO CITY          (359) 803-9809
##  3  1835 TURKISH AIRLINES ISTANBUL             (416) 788-2844
##  4   105 UNITED           WASHINGTON DC-DULLES (729) 609-4819
##  5  1973 CATHAY PACIFIC   HONG KONG            (201) 737-4409
##  6  2385 UNITED INTL      SYDNEY               (137) 611-3694
##  7   517 UNITED           FT. LAUDERDALE       (812) 869-6263
##  8  2885 EVA AIR          TAIPEI               (194) 198-0504
##  9  2128 FRONTIER         DENVER               (299) 137-6993
## 10  2132 FRONTIER         DENVER               (739) 710-2966
## # ... with 734 more rows</code></pre>
<pre class="r"><code># Remove rows with invalid numbers
phone_correction %&gt;%
  filter(str_length(phone) == 12)</code></pre>
<pre><code>## # A tibble: 2,065 x 4
##       id airline          destination phone       
##    &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;       &lt;chr&gt;       
##  1  1842 TURKISH AIRLINES ISTANBUL    858 990 5153
##  2  1844 TURKISH AIRLINES ISTANBUL    731-813-2043
##  3  1840 TURKISH AIRLINES ISTANBUL    563-732-6802
##  4  1837 TURKISH AIRLINES ISTANBUL    145 725 4021
##  5  1833 TURKISH AIRLINES ISTANBUL    931 311 5801
##  6  1838 TURKISH AIRLINES ISTANBUL    172 990 3485
##  7  1845 TURKISH AIRLINES ISTANBUL    872 325 4341
##  8  1846 TURKISH AIRLINES ISTANBUL    152 790 8238
##  9  1831 TURKISH AIRLINES ISTANBUL    330 561 9257
## 10  1848 TURKISH AIRLINES ISTANBUL    437 420 7546
## # ... with 2,055 more rows</code></pre>
</div>
<div id="date-uniformity" class="section level2">
<h2>3-2 Date uniformity</h2>
<p>In this chapter, you work at an asset management company and you’ll be working with the accounts dataset, which contains information about each customer, the amount in their account, and the date their account was opened. Your boss has asked you to calculate some summary statistics about the average value of each account and whether the age of the account is associated with a higher or lower account value. Before you can do this, you need to make sure that the accounts dataset you’ve been given doesn’t contain any uniformity problems. In this exercise, you’ll investigate the date_opened column and clean it up so that all the dates are in the same format.</p>
<p>dplyr and lubridate are loaded and accounts is available.</p>
<pre class="r"><code># Check out the accounts data frame
head(accounts)</code></pre>
<pre><code>##         id      date_opened    total
## 1 A880C79F       2003-10-19   169305
## 2 BE8222DF October 05, 2018   107460
## 3 19F9E113       2008-07-29 15297152
## 4 A2FE52A3       2005-06-09 14897272
## 5 F6DC2C08       2012-03-31   124568
## 6 D2E55799       2007-06-20 13635752</code></pre>
<pre class="r"><code># Define the date formats
formats &lt;- c(&quot;%Y-%m-%d&quot;, &quot;%B %d, %Y&quot;)

# Convert dates to the same format
accounts %&gt;%
  mutate(date_opened_clean = parse_date_time(date_opened, formats))</code></pre>
<pre><code>##          id        date_opened    total date_opened_clean
## 1  A880C79F         2003-10-19   169305        2003-10-19
## 2  BE8222DF   October 05, 2018   107460        2018-10-05
## 3  19F9E113         2008-07-29 15297152        2008-07-29
## 4  A2FE52A3         2005-06-09 14897272        2005-06-09
## 5  F6DC2C08         2012-03-31   124568        2012-03-31
## 6  D2E55799         2007-06-20 13635752        2007-06-20
## 7  53AE87EF  December 01, 2017 15375984        2017-12-01
## 8  3E97F253         2019-06-03 14515800        2019-06-03
## 9  4AE79EA1         2011-05-07 23338536        2011-05-07
## 10 2322DFB4         2018-04-07   189524        2018-04-07
## 11 645335B2         2018-11-16   154001        2018-11-16
## 12 D5EB0F00         2001-04-16   174576        2001-04-16
## 13 1EB593F7         2005-04-21   191989        2005-04-21
## 14 DDBA03D9         2006-06-13  9617192        2006-06-13
## 15 40E4A2F4         2009-01-07   180547        2009-01-07
## 16 39132EEA         2012-07-07 15611960        2012-07-07
## 17 387F8E4D   January 03, 2011  9402640        2011-01-03
## 18 11C3C3C0  December 24, 2017   180003        2017-12-24
## 19 C2FC91E1         2004-05-21   105722        2004-05-21
## 20 FB8F01C1         2001-09-06 22575072        2001-09-06
## 21 0128D2D0         2005-04-09 19179784        2005-04-09
## 22 BE6E4B3F         2009-10-20 15679976        2009-10-20
## 23 7C6E2ECC         2003-05-16   169814        2003-05-16
## 24 02E63545         2015-10-25   125117        2015-10-25
## 25 4399C98B       May 19, 2001   130421        2001-05-19
## 26 98F4CF0F       May 27, 2014 14893944        2014-05-27
## 27 247222A6       May 26, 2015   150372        2015-05-26
## 28 420985EE         2008-12-27   123125        2008-12-27
## 29 0E3903BA         2015-11-11   182668        2015-11-11
## 30 64EF994F         2009-02-26   161141        2009-02-26
## 31 CCF84EDB         2008-12-26   136128        2008-12-26
## 32 51C21705     April 22, 2016 16191136        2016-04-22
## 33 C868C6AD   January 31, 2000 11733072        2000-01-31
## 34 92C237C6         2005-12-13 11838528        2005-12-13
## 35 9ECEADB2       May 17, 2018   146153        2018-05-17
## 36 DF0AFE50         2004-12-03 15250040        2004-12-03
## 37 5CD605B3         2016-10-19    87921        2016-10-19
## 38 402839E2 September 14, 2019   163416        2019-09-14
## 39 78286CE7         2009-10-05 15049216        2009-10-05
## 40 168E071B         2013-07-11    87826        2013-07-11
## 41 466CCDAA         2002-03-24 14981304        2002-03-24
## 42 8DE1ECB9         2015-10-17   217975        2015-10-17
## 43 E19FE6B5      June 06, 2009   101936        2009-06-06
## 44 1240D39C September 07, 2011 15761824        2011-09-07
## 45 A7BFAA72         2019-11-12   133790        2019-11-12
## 46 C3D24436       May 24, 2002   101584        2002-05-24
## 47 FAD92F0F September 13, 2007 17081064        2007-09-13
## 48 236A1D51         2019-10-01 18486936        2019-10-01
## 49 A6DDDC4C         2000-08-17    67962        2000-08-17
## 50 DDFD0B3D         2001-04-11 15776384        2001-04-11
## 51 D13375E9  November 01, 2005 13944632        2005-11-01
## 52 AC50B796         2016-06-30 16111264        2016-06-30
## 53 290319FD       May 27, 2005   170178        2005-05-27
## 54 FC71925A  November 02, 2006   186281        2006-11-02
## 55 7B0F3685         2013-05-23   179102        2013-05-23
## 56 BE411172         2017-02-24 17689984        2017-02-24
## 57 58066E39 September 16, 2015 17025632        2015-09-16
## 58 EA7FF83A         2004-11-02 11598704        2004-11-02
## 59 14A2DDB7         2019-03-06 12808952        2019-03-06
## 60 305EEAA8         2018-09-01 14417728        2018-09-01
## 61 8F25E54C  November 24, 2008   189126        2008-11-24
## 62 19DD73C6         2002-12-31 14692600        2002-12-31
## 63 ACB8E6AF         2013-07-27    71359        2013-07-27
## 64 91BFCC40         2014-01-10   132859        2014-01-10
## 65 86ACAF81         2011-12-14 24533704        2011-12-14
## 66 77E85C14  November 20, 2009 13868192        2009-11-20
## 67 C5C6B79D         2008-03-01   188424        2008-03-01
## 68 0E5B69F5         2018-05-07 18650632        2018-05-07
## 69 5275B518         2017-11-23    71665        2017-11-23
## 70 17217048       May 25, 2001 20111208        2001-05-25
## 71 E7496A7F         2008-09-27   142669        2008-09-27
## 72 41BBB7B4  February 22, 2005   144229        2005-02-22
## 73 F6C7ABA1         2008-01-07   183440        2008-01-07
## 74 E699DF01  February 17, 2008   199603        2008-02-17
## 75 BACA7378         2005-05-11   204271        2005-05-11
## 76 84A4302F         2003-08-12 19420648        2003-08-12
## 77 F8A78C27     April 05, 2006    41164        2006-04-05
## 78 8BADDF6A  December 31, 2010   158203        2010-12-31
## 79 9FB57E68 September 01, 2017   216352        2017-09-01
## 80 5C98E8F5         2014-11-25   103200        2014-11-25
## 81 6BB53C2A  December 03, 2016   146394        2016-12-03
## 82 E23F2505   October 15, 2017   121614        2017-10-15
## 83 0C121914      June 21, 2017   227729        2017-06-21
## 84 3627E08A         2008-04-01   238104        2008-04-01
## 85 A94493B3    August 01, 2009    85975        2009-08-01
## 86 0682E9DE         2002-10-01    72832        2002-10-01
## 87 49931170         2011-03-25 14519856        2011-03-25
## 88 A154F63B         2000-07-11   133800        2000-07-11
## 89 3690CCED         2014-10-19   226595        2014-10-19
## 90 48F5E6D8  February 16, 2020   135435        2020-02-16
## 91 515FAD84         2013-06-20    98190        2013-06-20
## 92 59794264         2008-01-16   157964        2008-01-16
## 93 2038185B         2016-06-24   194662        2016-06-24
## 94 65EAC615  February 20, 2004   140191        2004-02-20
## 95 6C7509C9 September 16, 2000   212089        2000-09-16
## 96 BD969A9D         2007-04-29   167238        2007-04-29
## 97 B0CDCE3D       May 28, 2014   145240        2014-05-28
## 98 33A7F03E   October 14, 2007   191839        2007-10-14</code></pre>
</div>
<div id="currency-uniformity" class="section level2">
<h2>3-3 Currency uniformity</h2>
<p>Now that your dates are in order, you’ll need to correct any unit differences. When you first plot the data, you’ll notice that there’s a group of very high values, and a group of relatively lower values. The bank has two different offices - one in New York, and one in Tokyo, so you suspect that the accounts managed by the Tokyo office are in Japanese yen instead of U.S. dollars. Luckily, you have a data frame called account_offices that indicates which office manages each customer’s account, so you can use this information to figure out which totals need to be converted from yen to dollars.</p>
<p>The formula to convert yen to dollars is USD = JPY / 104.</p>
<p>dplyr and ggplot2 are loaded and the accounts and account_offices data frames are available.</p>
<pre class="r"><code># Scatter plot of opening date and total amount
accounts %&gt;%
  ggplot(aes(x = date_opened, y = total)) +
  geom_point()</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<pre class="r"><code># Left join accounts to account_offices by id
accounts %&gt;%
  left_join(account_offices, by = &quot;id&quot;) %&gt;%
  # Convert totals from the Tokyo office to USD
  mutate(total_usd = ifelse(office == &quot;Tokyo&quot;, total / 104, total)) %&gt;%
  # Scatter plot of opening date vs total_usd
  ggplot(aes(x = date_opened, y = total_usd)) +
    geom_point()</code></pre>
<p><img src="{{< blogdown/postref >}}index.en_files/figure-html/unnamed-chunk-20-2.png" width="672" /></p>
</div>
<div id="validating-totals" class="section level2">
<h2>3-5 Validating totals</h2>
<p>In this lesson, you’ll continue to work with the accounts data frame, but this time, you have a bit more information about each account. There are three different funds that account holders can store their money in. In this exercise, you’ll validate whether the total amount in each account is equal to the sum of the amount in fund_A, fund_B, and fund_C. If there are any accounts that don’t match up, you can look into them further to see what went wrong in the bookkeeping that led to inconsistencies.</p>
<p>dplyr is loaded and accounts is available.</p>
<pre class="r"><code># Find invalid totals
accounts_2 %&gt;%
  # theoretical_total: sum of the three funds
  mutate(theoretical_total=fund_A +fund_B +fund_C) %&gt;%
  # Find accounts where total doesn&#39;t match theoretical_total
  filter(theoretical_total!=total)</code></pre>
<pre><code>## # A tibble: 3 x 8
##   id       date_opened  total fund_A fund_B fund_C acct_age theoretical_total
##   &lt;chr&gt;    &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;             &lt;dbl&gt;
## 1 D5EB0F00 4/16/2001   130920  69487  48681  56408       20            174576
## 2 92C237C6 12/13/2005   85362  72556  21739  19537       15            113832
## 3 0E5B69F5 5/7/2018    134488  88475  44383  46475        3            179333</code></pre>
</div>
<div id="validating-age" class="section level2">
<h2>3-6 Validating age</h2>
<p>Now that you found some inconsistencies in the total amounts, you’re suspicious that there may also be inconsistencies in the acct_agecolumn, and you want to see if these inconsistencies are related. Using the skills you learned from the video exercise, you’ll need to validate the age of each account and see if rows with inconsistent acct_ages are the same ones that had inconsistent totals</p>
<p>dplyr and lubridate are loaded, and accounts is available.</p>
</div>
<div id="visualizing-missing-data" class="section level2">
<h2>3-9 Visualizing missing data</h2>
<p>Dealing with missing data is one of the most common tasks in data science. There are a variety of types of missingness, as well as a variety of types of solutions to missing data.</p>
<p>You just received a new version of the accounts data frame containing data on the amount held and amount invested for new and existing customers. However, there are rows with missing inv_amount values.</p>
<p>You know for a fact that most customers below 25 do not have investment accounts yet, and suspect it could be driving the missingness. The dplyr and visdat packages have been loaded and accounts is available.</p>
</div>
<div id="treating-missing-data" class="section level2">
<h2>3-10 Treating missing data</h2>
<p>In this exercise, you’re working with another version of the accounts data that contains missing values for both the cust_id and acct_amount columns.</p>
<p>You want to figure out how many unique customers the bank has, as well as the average amount held by customers. You know that rows with missing cust_id don’t really help you, and that on average, the acct_amount is usually 5 times the amount of inv_amount.</p>
<p>In this exercise, you will drop rows of accounts with missing cust_ids, and impute missing values of inv_amount with some domain knowledge. dplyr and assertive are loaded and accounts is available.</p>
</div>
<div id="small-distance-small-difference" class="section level2">
<h2>4-3 Small distance, small difference</h2>
<p>In the video exercise, you learned that there are multiple ways to calculate how similar or different two strings are. Now you’ll practice using the stringdist package to compute string distances using various methods. It’s important to be familiar with different methods, as some methods work better on certain datasets, while others work better on other datasets.</p>
<p>The stringdist package has been loaded for you.</p>
</div>
<div id="fixing-typos-with-string-distance" class="section level2">
<h2>4-4 Fixing typos with string distance</h2>
<p>In this chapter, one of the datasets you’ll be working with, zagat, is a set of restaurants in New York, Los Angeles, Atlanta, San Francisco, and Las Vegas. The data is from Zagat, a company that collects restaurant reviews, and includes the restaurant names, addresses, phone numbers, as well as other restaurant information.</p>
<p>The city column contains the name of the city that the restaurant is located in. However, there are a number of typos throughout the column. Your task is to map each city to one of the five correctly-spelled cities contained in the cities data frame.</p>
<p>dplyr and fuzzyjoin are loaded, and zagat and cities are available.</p>
</div>
<div id="pair-blocking" class="section level2">
<h2>4-7 Pair blocking</h2>
<p>Zagat and Fodor’s are both companies that gather restaurant reviews. The zagat and fodors datasets both contain information about various restaurants, including addresses, phone numbers, and cuisine types. Some restaurants appear in both datasets, but don’t necessarily have the same exact name or phone number written down. In this chapter, you’ll work towards figuring out which restaurants appear in both datasets.</p>
<p>The first step towards this goal is to generate pairs of records so that you can compare them. In this exercise, you’ll first generate all possible pairs, and then use your newly-cleaned city column as a blocking variable.</p>
<p>zagat and fodors are available.</p>
</div>
<div id="comparing-pairs" class="section level2">
<h2>4-8 Comparing pairs</h2>
<p>Now that you’ve generated the pairs of restaurants, it’s time to compare them. You can easily customize how you perform your comparisons using the by and default_comparator arguments. There’s no right answer as to what each should be set to, so in this exercise, you’ll try a couple options out.</p>
<p>dplyr and reclin are loaded and zagat and fodors are available.</p>
</div>
<div id="putting-it-together" class="section level2">
<h2>4-11 Putting it together</h2>
<p>During this chapter, you’ve cleaned up the city column of zagat using string similarity, as well as generated and compared pairs of restaurants from zagat and fodors. The end is near - all that’s left to do is score and select pairs and link the data together, and you’ll be able to begin your analysis in no time!</p>
<p>reclin and dplyr are loaded and zagat and fodors are available.</p>
</div>
